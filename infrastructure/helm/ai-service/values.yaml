# Helm values configuration for AI service deployment
# Version: 1.0.0

# Override names
nameOverride: "ai-service"
fullnameOverride: "ai-service"

# Container image configuration
image:
  repository: ai-service
  tag: ${VERSION}
  pullPolicy: Always

# Replica configuration per environment
replicaCount:
  dev: 2
  staging: 3
  prod: 4

# Service configuration
service:
  type: ClusterIP
  port: 8000
  targetPort: http
  protocol: TCP
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-internal: "true"
    service.beta.kubernetes.io/azure-load-balancer-internal-subnet: "ai-service-subnet"

# Deployment configuration
deployment:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
    prometheus.io/path: "/metrics"
    azure.workload.identity/use: "true"
    checksum/config: ${CONFIG_CHECKSUM}
    co.elastic.logs/enabled: "true"
  
  resources:
    requests:
      cpu: "1000m"
      memory: "2Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "2000m"
      memory: "4Gi"
      nvidia.com/gpu: "1"
  
  nodeSelector:
    kubernetes.io/os: linux
    accelerator: nvidia
    topology.kubernetes.io/region: ${REGION}
  
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - excel-addin
                - key: component
                  operator: In
                  values:
                    - ai
            topologyKey: kubernetes.io/hostname
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/gpu
                operator: Exists

# Autoscaling configuration
autoscaling:
  enabled: true
  minReplicas:
    dev: 2
    staging: 3
    prod: 4
  maxReplicas:
    dev: 4
    staging: 6
    prod: 8
  targetGPUUtilizationPercentage: 70
  targetCPUUtilizationPercentage: 80
  metrics:
    - type: Resource
      resource:
        name: nvidia.com/gpu
        targetAverageUtilization: 70
    - type: Resource
      resource:
        name: memory
        targetAverageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60

# Ingress configuration
ingress:
  enabled: false

# Service account configuration
serviceAccount:
  create: true
  name: "ai-service-account"
  annotations:
    azure.workload.identity/client-id: ${AZURE_CLIENT_ID}
    azure.workload.identity/tenant-id: ${AZURE_TENANT_ID}

# Security context configurations
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault

containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  capabilities:
    drop:
      - ALL

# Health check probes
livenessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 30
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: http
  initialDelaySeconds: 15
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

startupProbe:
  httpGet:
    path: /startup
    port: http
  initialDelaySeconds: 20
  periodSeconds: 10
  failureThreshold: 30

# Environment variables
env:
  PYTHONPATH: /app
  PYTHONUNBUFFERED: "1"
  PORT: "8000"
  LOG_LEVEL: INFO
  OPENAI_API_VERSION: "2023-05-15"
  MAX_BATCH_SIZE: "10"
  RESPONSE_TIMEOUT: "5"
  GPU_MEMORY_FRACTION: "0.9"